# ğŸ¥‰ QuickBite Lakehouse â€” Bronze Layer Documentation

**Phase 1: Raw Data Ingestion**

---

## ğŸ“Œ Overview

The **Bronze Layer** represents the **raw ingestion zone** of the QuickBite analytics platform.
Its purpose is to store source data **exactly as received** , without applying business logic or transformations.

In this project, the Bronze layer is implemented using **Microsoft Fabric Lakehouse (OneLake)** and serves as the foundation for all downstream Silver and Gold transformations.

---

## ğŸ¯ Objectives

- Ingest raw CSV extracts into OneLake
- Preserve original structure and values
- Enable schema discovery and query access via Delta tables
- Maintain immutability and traceability of source data

---

## ğŸ§° Components Used

| Component                        | Purpose                                       |
| -------------------------------- | --------------------------------------------- |
| **Fabric Lakehouse (OneLake)**   | Central storage for raw data files            |
| **Lakehouse UI (Manual Upload)** | Upload CSV files directly into Bronze folders |
| **Delta Tables (Bronze Tables)** | Register raw files as queryable tables        |

---

## ğŸ“¥ Ingestion Strategy

### Why Manual Upload?

Manual ingestion was chosen intentionally due to the **PoC nature** of this project.

**Rationale:**

- Small dataset size
- No requirement for scheduled or incremental ingestion
- Faster iteration during development
- Lower Fabric capacity usage

> âš ï¸ For production-scale pipelines, this approach should be replaced with Fabric Pipelines, Data Factory, or Dataflows Gen2.

---

## ğŸ—ï¸ Step 1 â€” Lakehouse & Bronze Folder Setup

### Lakehouse Created

- **Name:** `quickbite_lakehouse`

This Lakehouse acts as the central storage and processing layer for the project.

ğŸ“¸ _Screenshot:_
`/screenshots/quickbite_lakehouse.png`
![](../screenshots/quickbite_lakehouse.png)

---

### Bronze Folder Structure

Within the Lakehouse â†’ **Files** section, a dedicated `/bronze` directory was created to logically separate raw data.

```text
/bronze/
â”œâ”€â”€ fact_orders/
â”œâ”€â”€ fact_order_items/
â”œâ”€â”€ fact_ratings/
â”œâ”€â”€ fact_delivery_performance/
â”œâ”€â”€ dim_customer/
â”œâ”€â”€ dim_restaurant/
â”œâ”€â”€ dim_delivery_partner/
â””â”€â”€ dim_menu_item/
```

ğŸ“¸ _Screenshot:_
`/screenshots/quickbite_lakehouse_folder_str.png`
![](../screenshots/quickbite_lakehouse_folder_str.png)

Each folder corresponds to a **single source entity** , ensuring clarity and maintainability.

---

## ğŸ“¤ Step 2 â€” Upload Raw CSV Files

### Actions Performed

- Used **Lakehouse UI â†’ Upload â†’ Browse**
- Uploaded each CSV file into its corresponding Bronze folder
- No preprocessing or modification applied

### Key Principle

> **Raw means raw**
> Data in the Bronze layer is stored **as-is** , preserving the original source format and values.

---

## ğŸ“Š Step 3 â€” Create Bronze Delta Tables

### Objective

Convert raw CSV files into **Bronze Delta Tables** to enable:

- Schema inference
- SQL querying
- Downstream transformations

---

### Table Creation Process

For each CSV file:

**Navigation Path**

```
Lakehouse â†’ Files â†’ bronze â†’ <entity_folder>
â†’ Load to Tables â†’ New Table
```

ğŸ“¸ _Screenshot:_
`/screenshots/quickbite_lakehouse_bronze_table_creation.png`
![](../screenshots/quickbite_lakehouse_bronze_table_creation.png)

---

### Bronze Tables Created

```sql
bronze.fact_orders
bronze.fact_order_items
bronze.fact_ratings
bronze.fact_delivery_performance
bronze.dim_customer
bronze.dim_restaurant
bronze.dim_delivery_partner
bronze.dim_menu_item
```

### Schema Handling

- Column names auto-detected
- Basic data type coercion applied (string, integer, decimal)
- **No business transformations performed**

---

### Verification

All Bronze tables are visible under:

```
quickbite_lakehouse â†’ Tables â†’ bronze.*
```

ğŸ“¸ _Screenshot:_
`/screenshots/quickbite_lakehouse_bronze_tables_created.png`
![](../screenshots/quickbite_lakehouse_bronze_tables_created.png)

---

## âœ… Bronze Layer Best Practices Followed

- âœ” Raw data preserved without modification
- âœ” Logical folder-to-table mapping
- âœ” Clear separation between Facts and Dimensions
- âœ” Minimal processing (schema inference only)
- âœ” Ready for Silver layer transformations

---

## ğŸ”œ Next Phase

â¡ï¸ **Silver Layer**

- Data cleaning and standardization
- Dataflow Gen2 transformations
- Business-rule application

---

**Layer:** Bronze
**Platform:** Microsoft Fabric Lakehouse

---
